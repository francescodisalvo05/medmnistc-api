{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "\n",
    "The following notebook presents a rough outline of the required pipeline for robustness evaluation. <br />\n",
    "A runnable codebase with working experiments will be soon released!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from medmnistc.dataset import CorruptedMedMNIST\n",
    "from medmnistc.eval import Evaluator\n",
    "from medmnistc.corruptions.registry import CORRUPTIONS_DS\n",
    "\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(config, test_loader):\n",
    "    \"\"\"Placeholder for the inference pipeline\"\"\"\n",
    "    # load ckpt based on config\n",
    "    # ...\n",
    "\n",
    "    # inference\n",
    "    # ... \n",
    "\n",
    "    # return probabilities\n",
    "    pass\n",
    "\n",
    "\n",
    "config = {\n",
    "    '...' : '...' # define here all config results\n",
    "}\n",
    "\n",
    "# Load clean dataset\n",
    "test_dataset_clean = ... \n",
    "test_loader_clean = ...\n",
    "\n",
    "# Init the Evaluator class\n",
    "corruptions = CORRUPTIONS_DS[config['dataset']]\n",
    "evaluator = Evaluator(dataset_name=config['dataset'],\n",
    "                      true_labels=test_dataset_clean.labels,\n",
    "                      corruption_types=corruptions.keys(),\n",
    "                      output_folder=config['logs_path'],\n",
    "                      architecture=config['architecture'],\n",
    "                      task=config['task'],\n",
    "                      suffix_log=f\"s{config['seed']}\")\n",
    "\n",
    "\n",
    "# Evaluate performance on clean test set (to work with the relative corruptions)\n",
    "print(\"\\tEvaluating performance on the clean test set...\")\n",
    "\n",
    "y_pred = inference(config, test_loader_clean)\n",
    "\n",
    "\n",
    "evaluator.evaluate_clean(y_pred.cpu().numpy())\n",
    "\n",
    "# Iterate over the designed corruptions.\n",
    "for corruption in corruptions.keys():\n",
    "\n",
    "    print(corruption)\n",
    "    \n",
    "    # Load the corrupted test set, according to the selected corruption\n",
    "    corrupted_test_test = CorruptedMedMNIST(\n",
    "                                dataset_name = config['dataset'], \n",
    "                                corruption = corruption,\n",
    "                                root = config['dataset_corrupted_path'],\n",
    "                                mmap_mode='r')\n",
    "    \n",
    "    # Get dataloader\n",
    "    test_loader = DataLoader(corrupted_test_test, batch_size=config['batch_size_eval'], shuffle=False)\n",
    "\n",
    "    # Evaluate\n",
    "    y_pred = inference(config, test_loader)     \n",
    "\n",
    "    # Calculate the error\n",
    "    evaluator.evaluate(y_pred.cpu().numpy(), corruption)\n",
    "\n",
    "\n",
    "# Create a json file containing the aggregated and raw results\n",
    "evaluator.dump_summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medmnistc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
